{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Starter Notebook: Your First LLM with vLLM\n\nWelcome! This notebook is written for complete beginners.\n\nBy the end, you will know how to:\n1. Install and import **vLLM**.\n2. Load a **small instruction model**.\n3. Build a prompt with a **chat template** (including a **system message**).\n4. Generate text and **extract model outputs** clearly.\n\n---\n\n> **Who this is for:** People with little to no prior LLM experience."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 1 — (Optional) Install dependencies\n\nIf you're running this in a fresh environment, run this cell once.\n\n- `vllm` is the serving/inference engine.\n- `transformers` provides tokenizer + chat template utilities.\n\n> If you already installed these packages, you can skip this cell."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 1: Install dependencies (optional)\n# Remove the leading \"!\" and run in Jupyter/Colab.\n!pip install -q vllm transformers"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 2 — Imports and quick environment check\n\nThis cell imports what we need and prints useful version information."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 2: Imports\nimport torch\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 3 — Choose a small model\n\nWe'll use a small instruction-tuned model:\n\n- **Model:** `Qwen/Qwen2-0.5B-Instruct`\n\nWhy this one?\n- It's small enough to be beginner-friendly.\n- It supports chat-style prompting.\n\n> If your machine is very limited, you may need to try an even smaller or quantized model."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 3: Pick model name\nmodel_name = \"Qwen/Qwen2-0.5B-Instruct\"\nprint(f\"Using model: {model_name}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 4 — Load tokenizer and vLLM model\n\nThis can take a little time on first run because model files are downloaded.\n\n- `AutoTokenizer` handles chat templates.\n- `LLM(...)` loads the model with vLLM."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 4: Load tokenizer + model\n# Tip: You can adjust tensor_parallel_size for multi-GPU setups.\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nllm = LLM(\n    model=model_name,\n    trust_remote_code=True,  # often needed for some model/tokenizer repos\n)\n\nprint(\"Tokenizer and model loaded successfully.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 5 — Create chat messages (system + user)\n\nA **system message** sets global behavior for the assistant.\nA **user message** is the actual question/task."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 5: Define messages\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": (\n            \"You are a helpful tutor for beginners. \"\n            \"Explain concepts clearly using short examples.\"\n        ),\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"In 3 bullets, what is an LLM and what can it do?\",\n    },\n]\n\nmessages"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 6 — Apply the chat template\n\n`apply_chat_template(...)` converts role-based messages into the model's expected text format.\n\nImportant options:\n- `tokenize=False`: return a string prompt.\n- `add_generation_prompt=True`: append the assistant turn so the model knows to answer next."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 6: Build prompt from chat messages\nprompt_text = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n\nprint(\"Prompt sent to the model:\")\nprint(\"-\" * 80)\nprint(prompt_text)\nprint(\"-\" * 80)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 7 — Set generation parameters\n\nThese control how the model writes:\n- `temperature`: creativity/randomness (higher = more varied).\n- `max_tokens`: maximum length of generated output."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 7: Define sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.7,\n    max_tokens=200,\n)\n\nsampling_params"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 8 — Generate output with vLLM\n\nWe call `llm.generate(...)` with a list of prompts.\nEven for one prompt, we pass a list (this supports batching)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 8: Generate response\nresults = llm.generate([prompt_text], sampling_params)\n\ntype(results), len(results)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 9 — Extract and print the assistant text\n\nvLLM returns nested objects. For one prompt + one completion:\n- `results[0]` = first prompt's output object\n- `results[0].outputs[0].text` = generated assistant text"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 9: Extract generated text\nassistant_text = results[0].outputs[0].text\n\nprint(\"Assistant output:\")\nprint(assistant_text)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 10 — (Optional) Inspect output metadata\n\nThis is useful if you want more than just text (for logging, debugging, evaluation)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Cell 10: Inspect useful metadata\nfirst_result = results[0]\nfirst_completion = first_result.outputs[0]\n\nprint(f\"Prompt token IDs length: {len(first_result.prompt_token_ids)}\")\nprint(f\"Generated token IDs length: {len(first_completion.token_ids)}\")\nprint(f\"Finish reason: {first_completion.finish_reason}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cell 11 — Next steps (ideas)\n\nTry changing one thing at a time:\n1. Replace the user question.\n2. Change the system message style.\n3. Lower `temperature` to `0.2` for more deterministic answers.\n4. Increase `max_tokens` for longer outputs.\n\nYou're now using vLLM with chat templates end-to-end ✅"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}